# find_centroids_gravity_center_recursive

整体逻辑是为了在一个二进制掩码中识别并调整大区域的质心，确保这些质心位于其对应的区域内。这里的"大区域"是指面积大于总掩码面积的给定比例的区域。

具体的步骤如下：

### 区域标记：

- 使用标记函数对掩码中的各个连续区域进行标记，这样每个区域都有一个独特的标签。

### 定义大区域：

- 根据提供的比例，计算出一个面积阈值。
- 对于所有区域，识别那些面积大于这个阈值的区域，认为这些是"大区域"。

### 质心检查与调整：

- 对于每个大区域：
  - 首先计算其质心（也称为重心或质量中心）。
  - 检查这个质心是否位于其对应的区域内。
  - 如果质心在区域内，则不需要进一步的操作。
  - 如果质心不在区域内或者恰好在边界上，这时会进行调整：
    - 找到距离质心最近的边界点。
    - 计算从质心到这个边界点的方向矢量。
    - 如果质心在边界上，则计算一个与该方向垂直的矢量。
    - 使用上述矢量，沿着从质心开始的方向切割这个区域。
    - 对切割后得到的新区域，再次进行质心的计算和调整。
  - 注意，这个调整过程是递归的。为了避免无限递归，设置了一个最大递归深度。

### 结果输出：

- 返回所有大区域的标签和它们对应的质心。

整个过程的目的是确保每个大区域都有一个或多个质心，而这些质心都位于其对应的区域内。这对于图像处理和计算机视觉任务中的区域描述和特征提取非常有用。

# find_centroids_gravity_center_adjusted

总体逻辑概述： 这些函数的目的是从一个二进制图像中找到并调整大区域的重心，确保每个重心都位于其对应的区域内部。如果初步计算的重心不在区域内，我们需要找到一个与重心最近的边界点，并确定从重心到该点的方向。然后，我们需要找到这一方向上与区域的交点，最后将这些交点的平均位置作为调整后的重心。

详细步骤：

### 标记和定义大区域：

- 使用 label 函数标记掩码中的各个连续区域。
- 计算每个区域的面积，并根据给定的比例计算出面积阈值。
- 根据面积阈值来识别那些被视为"大区域"的区域。

### 计算和调整大区域的重心：

- 遍历每个大区域，对于每个区域：
  - 使用 center_of_mass 计算其质量中心，这个质量中心被视为初始的重心。
  - 使用 adjusted_centroid 函数确保重心位于其对应的区域内部。
    - 首先，查找区域的轮廓。
    - 检查初始重心是否位于区域内。
    - 如果不在内部，则进行调整：
      - 使用 closest_boundary_point 函数找到距离初始重心最近的边界点。
      - 计算从初始重心到该边界点的方向矢量。
      - 使用 intersection_points 函数找到这一方向上与区域的交点。
      - 如果找到的交点至少有两个，计算这些交点的平均位置作为调整后的重心。

### 返回结果：

- 返回所有大区域的标签和它们对应的调整后的重心。

通过上述步骤，这些函数确保了每个大区域都有一个调整后的重心，且这个重心确实位于其对应的区域内部。

# find_centroids_shrink_mask

整体逻辑的核心是：对于二进制掩码中的每个大连通区域，我们希望找到一个代表其中心的点。为了找到这个中心点，我们采用了一个不断侵蚀区域直到其几乎消失的方法。

详细步骤如下：

### 侵蚀掩码以找到中心点 (shrink_mask 函数)：

- 输入：一个连通区域的二进制掩码。
- 输出：该连通区域中心的位置。
- 流程：
  - 首先，我们备份当前掩码，以便在后续步骤中，如果掩码完全被侵蚀（变空）时可以恢复。
  - 我们开始一个循环，只要掩码中还有多于一个的非零像素，就继续。
  - 在每次循环中，我们使用半径为1的磁盘结构元素对掩码进行侵蚀。
  - 我们检查侵蚀后的掩码是否为空。如果为空，我们知道上一次的掩码只剩下最后的中心点，所以我们恢复上一次的掩码。
  - 在结束循环后，我们返回掩码中仅存的像素位置作为中心点。

### 为大连通区域找到并调整中心 (find_centroids_shrink_mask 函数)：

- 输入：一个二进制掩码和一个面积比例阈值。
- 输出：大连通区域的标签和每个区域的中心位置。
- 流程：
  - 使用 label 函数将输入掩码中的连通区域进行编号。
  - 计算每个连通区域的面积。
  - 计算整个掩码的总面积。
  - 根据给定的比例，计算出一个面积阈值，用于确定哪些连通区域是“大”连通区域。
  - 根据这个阈值，我们筛选出所有面积大于该阈值的连通区域。
  - 对于每个筛选出的大连通区域：
    - 提取该区域的二进制掩码。
    - 使用 shrink_mask 函数对这个区域进行侵蚀，直到我们找到其中心点。
    - 将这个中心点加入到中心列表中。
  - 函数最后返回所有大连通区域的标签和它们对应的中心位置。

综合概述： 这套函数的主要目标是为二进制掩码中的大连通区域找到代表性的中心点。通过不断侵蚀每个大连通区域，直到它几乎消失，我们可以确定一个中心点，这个点可以很好地代表整个区域。

# SAM：

SAM的predict方法接受输入为：

- **point_coords (np.ndarray or None)：**
  - 类型：N x 2 的数组。
  - 描述：提供给模型的点提示。每个点的坐标为 (X,Y) 格式，表示像素位置。
- **point_labels (np.ndarray or None)：**
  - 类型：长度为 N 的数组。
  - 描述：点提示的标签。其中，1 表示前景点，0 表示背景点。
- **box (np.ndarray or None)：**
  - 类型：长度为 4 的数组。
  - 描述：给模型的方框提示，格式为 XYXY。
- **mask_input (np.ndarray)：**
  - 类型：1 x H x W 的数组，通常情况下，对于 SAM，H=W=256。
  - 描述：给模型的低分辨率掩码输入，通常来源于之前的预测迭代。
- **multimask_output (bool)：**
  - 类型：布尔值。
  - 描述：如果为真，模型将返回三个掩码。对于不明确的输入提示（例如单个点击），与单个预测相比，这通常会产生更好的掩码。
- **return_logits (bool)：**
  - 类型：布尔值。
  - 描述：如果为真，则返回未经阈值处理的掩码对数几率，而不是二值掩码。

在实验中，给入的输入为前述方法找到的前景点或前景点和背景点、无方框提示、unet初步预测（经过4x4的Maxpooling，缩小为256*256）、单个预测、阈值化处理。

- **预训练模型大小：**
  - ViT-L

# 数据集大小：

    | Training Percentage |     | M1  | M2  | M3 | T1  | T2  | T3 |
    |---------------------|-----|-----|-----|----|-----|-----|----|
    | 100%                |     | 177 | 207 | 90 | 192 | 192 | 54 |
    | 80%                 |     | 141 | 165 | 72 | 153 | 152 | 42 |
    | 60%                 |     | 105 | 123 | 54 | 114 | 114 | 30 |
    | 40%                 |     | 69  | 123 | 36 | 75  | 75  | 21 |
    | 20%                 |     | 33  | 78  | 18 | 36  | 36  | 9  |

    | Dataset | M1 | M2 | M3 | T1 | T2 | T3 |
    |---------|----|----|----|----|----|----|
    | Test    | 37 | 45 | 19 | 37 | 40 | 12 |

# UNet 参数：

    | 层名称    | 输入大小                | 输出大小                |
    |----------|------------------------|------------------------|
    | enc1     | 1024 x 1024 x 3        | 1024 x 1024 x 64       |
    | maxpool  | 1024 x 1024 x 64       | 512 x 512 x 64         |
    | enc2     | 512 x 512 x 64         | 512 x 512 x 128        |
    | maxpool  | 512 x 512 x 128        | 256 x 256 x 128        |
    | enc3     | 256 x 256 x 128        | 256 x 256 x 256        |
    | maxpool  | 256 x 256 x 256        | 128 x 128 x 256        |
    | enc4     | 128 x 128 x 256        | 128 x 128 x 512        |
    | maxpool  | 128 x 128 x 512        | 64 x 64 x 512          |
    | bridge   | 64 x 64 x 512          | 64 x 64 x 1024         |
    | up1      | 64 x 64 x 1024         | 128 x 128 x 512        |
    | dec1     | 128 x 128 x 1024       | 128 x 128 x 512        |
    | up2      | 128 x 128 x 512        | 256 x 256 x 256        |
    | dec2     | 256 x 256 x 512        | 256 x 256 x 256        |
    | up3      | 256 x 256 x 256        | 512 x 512 x 128        |
    | dec3     | 512 x 512 x 256        | 512 x 512 x 128        |
    | up4      | 512 x 512 x 128        | 1024 x 1024 x 64       |
    | dec4     | 1024 x 1024 x 128      | 1024 x 1024 x 64       |
    | out      | 1024 x 1024 x 64       | 1024 x 1024 x out_channels |

    ## UNet 架构总结

UNet 是一个专为图像分割设计的深度学习架构。其特点是对称结构，以及编码器和解码器之间的跳跃连接。下面是其主要组件的详细描述：

### 1. 编码器 (Encoder)

- 输入图像首先经过编码器。
- 包含 **4 个卷积块**，每个块由两个连续的卷积层组成。
- 每次通过一个卷积块后，都会进行一次**最大池化**以减少空间维度。
- 输出通道数量在每个块中都会翻倍，从 64 到 512。

### 2. 桥接 (Bridge)

- 在编码器和解码器之间的部分。
- 由一个**卷积块**组成，其输入和输出通道数都是 1024。

### 3. 解码器 (Decoder)

- 用于逐步将特征图的尺寸增大，从而恢复到与输入图像相同的尺寸。
- 包含 **4 个卷积块**，与编码器中的块结构相同。
- 每个块前都有一个**转置卷积层（upsampling）**来增大特征图的尺寸。
- 在每个 upsampling 之后，都会将对应的编码器输出与解码器的当前特征图进行**拼接**。这种跳跃连接确保了细节信息能够从编码器直接传递到解码器。
- 输出通道数量从 512 减少到 64。

### 4. 输出层

- 最后是一个卷积层，用于将解码器的输出转化为**最终的分割掩码**。
- 输出通道数与所需的掩码类别数相同。

# Transform:

            transforms.RandomAffine(20, translate=(0.1, 0.1)),
            transforms.RandomHorizontalFlip(0.5),
            transforms.RandomVerticalFlip(0.5),

# 损失函数总结

在给定的代码中，使用了两种主要的损失函数来训练模型。

### 1. 交叉熵损失 (Binary Cross Entropy Loss)

- **Function**: `ce_loss(pred, target)`
- **Description**: 
  交叉熵损失是用于二分类问题的常见损失函数。它衡量了模型的预测概率分布与真实分布之间的距离。在这里，使用了带有逻辑函数的二进制交叉熵损失，因此可以直接处理模型的原始输出。

### 2. IOU 损失 (Intersection over Union Loss)

- **Function**: `iou_loss(pred, target)`
- **Description**: 
  IOU 损失函数是一个用于衡量预测掩码与真实掩码重叠度的函数。IOU 值的范围在 0 到 1 之间，其中 1 表示完美的重叠。IOU 损失则是 \(1 - \text{IOU}\)，因此当预测与目标越接近时，损失越小。
  这里的实现可以处理 3D 或 4D 的输入，这意味着它可以计算整个批次的 IOU 或每个样本的 IOU 并对它们进行平均。

### 训练的loss为 loss = ce_loss + iou_loss
### 验证的loss为 loss = ce_loss + iou_loss(pre>0.5) (阈值化之后的IOU)

# 硬件：

RWTH HPC GPU Cluster  
2 x Tesla V100 per Node

